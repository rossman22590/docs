---
title: "Streaming Chat"
description: "Build your chatbot UI using our streaming chat endpoint, which is fully compatible with Vercel AI SDKs such as useChat.
"
openapi: "POST /api/v1/chat/token/stream"
---


### AI Tutor API supports real-time streaming responses, enabling progressive data retrieval for enhanced user experience. This feature is particularly valuable for applications requiring immediate feedback and interactive learning experiences.

Client-side streaming in AI Tutor API allows for efficient data transmission, displaying results as they become available, making it ideal for real-time tutoring sessions and interactive educational content delivery.

#### Key Components of Streaming Implementation

<CardGroup cols={2}>
  <Card title="Single-Use Token Authentication" icon="key">
    Secure your requests with **single-use tokens** for client-side authentication.
  </Card>

  <Card title="Real-time Data Streaming" icon="stream">
    Receive and display **progressive responses** as the AI tutor generates content.
  </Card>

  <Card title="Client-side Integration" icon="browser">
    Easy-to-implement **React components** for handling streaming responses.
  </Card>

  <Card title="Flexible Input Processing" icon="input-text">
    Support for **various input formats** in streaming requests.
  </Card>
</CardGroup>

#### Implementation Guide

<AccordionGroup>
  <Accordion title="Step 1: Generate Authentication Token" icon="key">
    First, obtain a single-use token from the authentication endpoint for secure client-side requests.
    
    ```bash
    curl --request POST \
      --url https://aitutor-api.vercel.app/api/auth/token




</Accordion> <Accordion title="Step 2: Make Streaming Request" icon="code"> Use the token to make a streaming request to the AI Tutor API:
bash


    curl --request POST \
      --url https://aitutor-api.vercel.app/api/v1/stream?token=your_token_here \
      --header 'Content-Type: application/json' \
      --data '{"prompt":"your_learning_prompt"}'




</Accordion> <Accordion title="Step 3: Implement React Component" icon="react"> Use this React component to handle streaming responses:
typescript


    "use client";

    import { useCallback, useEffect, useState } from "react";
    import { Spinner } from "./loaders";

    export default function StreamingText({
      url,
      fallbackText,
      className,
    }: {
      url: string;
      fallbackText: string;
      className?: string;
    }) {
      const [loading, setLoading] = useState(false);
      const [result, setResult] = useState("");

      const getData = useCallback(async () => {
        setLoading(true);
        const response = await fetch(url, {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
        });

        if (!response.ok) {
          return null;
        }

        const data = response.body;
        if (!data) {
          setResult(fallbackText);
          return;
        }

        const reader = data.getReader();
        const decoder = new TextDecoder();
        let done = false;

        setLoading(false);

        while (!done) {
          const { value, done: doneReading } = await reader.read();
          done = doneReading;
          const chunkValue = decoder.decode(value);
          setResult((prev) => (prev ?? "") + chunkValue);
        }
      }, [url, fallbackText]);

      useEffect(() => {
        if (url) {
          getData();
        }
      }, []);

      return loading ? (
        <Spinner className={className} />
      ) : (
        <p className={className}>{result}</p>
      );
    }





</Accordion> <Accordion title="Step 4: Handle Responses" icon="gear"> Process streaming responses in your application with proper error handling and UI updates. </Accordion> </AccordionGroup>
Best Practices for Streaming Implementation
<AccordionGroup> <Accordion title="Error Handling" icon="shield-check"> Implement **robust error handling** for network issues and invalid responses. </Accordion> <Accordion title="Loading States" icon="spinner"> Display appropriate **loading indicators** while waiting for initial response. </Accordion> <Accordion title="Token Management" icon="key-skeleton"> Properly manage **single-use tokens** and implement refresh mechanisms. </Accordion> <Accordion title="Performance Optimization" icon="gauge"> Optimize your application to handle **continuous data streams** efficiently. </Accordion> </AccordionGroup>
Benefits of Streaming in AI Tutor API
Real-time Feedback : Immediate display of AI tutor responses as they're generated.

Enhanced UX : Better user experience with progressive content loading.

Resource Efficiency : Optimized data transmission and processing.

Interactive Learning : Support for real-time interactive tutoring sessions.

Scalability : Efficient handling of large responses through chunked transfer.